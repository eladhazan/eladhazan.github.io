---
layout: page
title: Open Problem
permalink: /openproblem/
---

### **Open Problem in Optimization**


The rise of [adaptive gradient methods](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf) has been a meteoric success. How can we explain their performance gains compared to vanilla stochastic gradient descent? 
Some [theoretical arguments](https://arxiv.org/abs/1705.08292) point to ineffectiveness, but their prevailence in pratice is undisputed.

In this [open problem](https://ehazan.com/open_problem.pdf), jointly written with [Xinyi Chen](https://xinyi.github.io/), we lay out a rigorous direction to explain the performance improvements of adaptive gradient methods. If succecssful, a solution of the open problem will explain improvements in training of deep neural networks. The convergence speed vs. that of stochastic gradient descent can be up to square root of the number of parameters in the neural net which is being trained.

### **The Prize**

A prize of 500$ will be given to the successful solver/solvers, for either proof or disproof of the conjecture. 

